{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "387bd5ff",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a2320a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from re import search\n",
    "from typing import List, Tuple, Any, Union\n",
    "import nltk\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer, AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "import collections\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "\n",
    "def load_json(file: str):\n",
    "    return json.loads(Path(file).read_bytes())\n",
    "\n",
    "# Load data\n",
    "p_data = pd.read_csv(\"./dataset/train.csv\").drop([\"Unnamed: 6\", \"total no.: 7987\"], axis=1)\n",
    "split_ids = load_json('./dataset/splitIds__splitBy-id_stratifyBy-s_train-0.6_valid-0.2_test-0.2_seed-42.json')\n",
    "train_data, valid_data, train2_data = [p_data[p_data.id.isin(split_ids[split])] for split in [\"train\", \"valid\", \"test\"]]\n",
    "\n",
    "# Concat \"train\" and \"test\" split into new training data \n",
    "train_data = pd.concat([train_data, train2_data],axis=0)\n",
    "print(train_data.shape[0], valid_data.shape[0])\n",
    "p_data = pd.read_csv(\"./dataset/train.csv\").drop([\"Unnamed: 6\", \"total no.: 7987\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52386a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the parameters of the best model\n",
    "# The total batch size is 32 (8 * 2(accumulate) * 2(two gpus))\n",
    "args = {\n",
    "    \"max_len\" : 512,\n",
    "    \"batch_size\" : 8,\n",
    "    \"model_name\" : \"janeel/muppet-roberta-base-finetuned-squad\",\n",
    "    \"learning_rate\" : 3e-5,\n",
    "    \"warmup_ratio\" : 0.06,\n",
    "    \"seed\" : 26,\n",
    "    \"split\" : \"6+22\",\n",
    "    \"special\" : \"shuffle\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128c648c",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1d09d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains(small, big):\n",
    "    for i in range(len(big)-len(small)+1):\n",
    "        for j in range(len(small)):\n",
    "            if big[i+j] != small[j]:\n",
    "                break\n",
    "        else:\n",
    "            return i, i+len(small)-1\n",
    "    return False\n",
    "\n",
    "def keep_continuous(data: pd.DataFrame):\n",
    "    keep=[]\n",
    "    for i in range(data.shape[0]):\n",
    "        qp_not_in_q = data.iloc[i]['q\\''][1:-1] not in data.iloc[i]['q'][1:-1]\n",
    "        rp_not_in_r = data.iloc[i]['r\\''][1:-1] not in data.iloc[i]['r'][1:-1]\n",
    "        if not (qp_not_in_q or rp_not_in_r):\n",
    "            keep.append(i)\n",
    "    \n",
    "    data = data.iloc[keep]\n",
    "    return data\n",
    "\n",
    "def format_data_qp(q: List[int], r: List[int], s: int, qp: List[int], rp: List[int]) -> Tuple[List[int], List[int], List[int], int, int]:\n",
    "    q_r_s = [clsid] + q + [sepid] + r + [sepid] + s + [sepid]\n",
    "    attention_mask = [1 if _ in range(len(q_r_s)) else 0 for _ in range(args['max_len'])]\n",
    "    input_id = [q_r_s[_] if _ in range(len(q_r_s)) else padid for _ in range(args['max_len'])]\n",
    "    \n",
    "    if contains(qp, q_r_s):\n",
    "        start_pos, end_pos = contains(qp, q_r_s)\n",
    "        print('qp:', qp)\n",
    "        print('q_r_s:', q_r_s)\n",
    "        print(start_pos, end_pos)\n",
    "    else:\n",
    "        start_pos, end_pos = 0, 0\n",
    "        \n",
    "    return input_id, attention_mask, start_pos, end_pos\n",
    "\n",
    "def format_data_rp(q: List[int], r: List[int], s: int, qp: List[int], rp: List[int]) -> Tuple[List[int], List[int], List[int], int, int]:\n",
    "    q_r_s = [clsid] + r + [sepid] + q + [sepid] + s + [sepid]\n",
    "    attention_mask = [1 if _ in range(len(q_r_s)) else 0 for _ in range(args['max_len'])]\n",
    "    input_id = [q_r_s[_] if _ in range(len(q_r_s)) else padid for _ in range(args['max_len'])]\n",
    "    \n",
    "    if contains(rp, q_r_s):\n",
    "        start_pos, end_pos = contains(rp, q_r_s)\n",
    "    else:\n",
    "        start_pos, end_pos = 0, 0\n",
    "        \n",
    "    return input_id, attention_mask, start_pos, end_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd22a822",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1429813",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(args[\"model_name\"])\n",
    "clsid = tokenizer.cls_token_id\n",
    "sepid = tokenizer.sep_token_id\n",
    "padid = tokenizer.pad_token_id\n",
    "def model_tokenize(text: str) -> List[int]:\n",
    "    text = text.strip('\"')\n",
    "    token_ids = tokenizer(text)[\"input_ids\"]\n",
    "    return token_ids[1:-1] #without cls sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fe7253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data: pd.DataFrame, choice:str):\n",
    "    # Only keep data with continuous span\n",
    "    data = keep_continuous(data)\n",
    "    print('ids left:', data['id'].nunique())\n",
    "    print('instances left', data.shape[0])\n",
    "    ids = list(data.id)\n",
    "    Q, R, S, QP, RP = [data[field] for field in [\"q\", \"r\", \"s\", \"q'\", \"r'\"]]\n",
    "    Q, R, QP, RP, S = [list(map(model_tokenize, x)) for x in [Q, R, QP, RP, S]]\n",
    "\n",
    "    # Only keep data Q+R+S < 512 tokens\n",
    "    count = 0\n",
    "    keep = []\n",
    "    for i in range(len(Q)):\n",
    "        if (len(Q[i])+len(R[i])) > 512-5:\n",
    "            count += 1\n",
    "        else:\n",
    "            keep.append(i)\n",
    "    print(f\"Q+R+S longer than {args['max_len']} tokens:\", count, \" Remains:\",len(keep))\n",
    "    Q = [Q[i] for i in keep]\n",
    "    R = [R[i] for i in keep]\n",
    "    QP = [QP[i] for i in keep]\n",
    "    RP = [RP[i] for i in keep]\n",
    "    S = [S[i] for i in keep]\n",
    "    ids = [ids[i] for i in keep]\n",
    "    \n",
    "    # Find start end positions then make a dictionary\n",
    "    if choice == 'qp':\n",
    "        data = list(map(format_data_qp, Q, R, S, QP, RP))\n",
    "    elif choice == 'rp':\n",
    "        data = list(map(format_data_rp, Q, R, S, QP, RP))\n",
    "    else:\n",
    "        return 'ERROR'\n",
    "    return 0\n",
    "    input_list, token_list, attention_list, s_pos, e_pos =[], [], [], [], []\n",
    "    for i in range(len(data)):\n",
    "        input_list.append(data[i][0])\n",
    "        attention_list.append(data[i][1])\n",
    "        s_pos.append(data[i][2])\n",
    "        e_pos.append(data[i][3])\n",
    "        \n",
    "    data = {\n",
    "        'input_ids': input_list,\n",
    "        'attention_masks': attention_list,\n",
    "        'start_positions': s_pos,\n",
    "        'end_positions': e_pos\n",
    "    }\n",
    "    \n",
    "    # Turn dictionary into a dataset\n",
    "    ds = Dataset.from_dict(data)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9499ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start preprocess\n",
    "train_data_qp_done=preprocess(train_data, 'qp')\n",
    "valid_data_qp_done=preprocess(valid_data, 'qp')\n",
    "\n",
    "train_data_rp_done=preprocess(train_data, 'rp')\n",
    "valid_data_rp_done=preprocess(valid_data, 'rp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541aeecd",
   "metadata": {},
   "source": [
    "### Model / Collator / Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7980f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "from transformers import default_data_collator\n",
    "\n",
    "# Start training Q'\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(args[\"model_name\"])\n",
    "model_args = TrainingArguments(\n",
    "    f'corrected_models/{args[\"model_name\"]}-qp-{args[\"split\"]}-b_{args[\"batch_size\"]}-lr_{args[\"learning_rate\"]}-warm_{args[\"warmup_ratio\"]}-seed_{args[\"seed\"]}-{args[\"special\"]}',\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=args[\"learning_rate\"],\n",
    "    per_device_train_batch_size=args['batch_size'],\n",
    "    per_device_eval_batch_size=args['batch_size'],\n",
    "    warmup_ratio=args[\"warmup_ratio\"],\n",
    "    seed=args[\"seed\"],\n",
    "    num_train_epochs=6,\n",
    "    weight_decay=0.01,\n",
    "    gradient_accumulation_steps=2\n",
    ")\n",
    "data_collator = default_data_collator\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    model_args,\n",
    "    train_dataset=train_data_qp_done,\n",
    "    eval_dataset=valid_data_qp_done,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efea988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training R'\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(args[\"model_name\"])\n",
    "model_args = TrainingArguments(\n",
    "    f'corrected_models/{args[\"model_name\"]}-rp-{args[\"split\"]}-b_{args[\"batch_size\"]}-lr_{args[\"learning_rate\"]}-warm_{args[\"warmup_ratio\"]}-seed_{args[\"seed\"]}-{args[\"special\"]}',\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=args[\"learning_rate\"],\n",
    "    per_device_train_batch_size=args['batch_size'],\n",
    "    per_device_eval_batch_size=args['batch_size'],\n",
    "    warmup_ratio=args[\"warmup_ratio\"],\n",
    "    seed=args[\"seed\"],\n",
    "    num_train_epochs=6,\n",
    "    weight_decay=0.01,\n",
    "    gradient_accumulation_steps=2\n",
    ")\n",
    "data_collator = default_data_collator\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    model_args,\n",
    "    train_dataset=train_data_rp_done,\n",
    "    eval_dataset=valid_data_rp_done,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0089f4",
   "metadata": {},
   "source": [
    "### Predict Answer from model checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c42c1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_unique_id(data: pd.DataFrame):\n",
    "    ids = []\n",
    "    index = []\n",
    "    for i in range(data.shape[0]):\n",
    "        if data.iloc[i]['id'] not in ids:\n",
    "            ids.append(data.iloc[i]['id'])\n",
    "            index.append(i)\n",
    "    print(len(index), len(ids))\n",
    "    data = data.iloc[index]\n",
    "    return data\n",
    "\n",
    "def format_data_post_qp(q: str, r: str, s: str, ids: str):\n",
    "    q_r_s =  q + '</s>' + r + '</s>' + s \n",
    "    tokenized_q_r_s = tokenizer(q_r_s, return_offsets_mapping=True, padding=\"max_length\", max_length=512, truncation=True)\n",
    "    \n",
    "    cls_q_sep =  q\n",
    "    tokenized_q = tokenizer(cls_q_sep)[\"input_ids\"]\n",
    "    \n",
    "    tokenized_q_r_s[\"example_id\"] = ids\n",
    "    tokenized_q_r_s[\"offset_mapping\"] = [tokenized_q_r_s[\"offset_mapping\"][_] if _ in range(len(tokenized_q)-1) else None for _ in range(len(tokenized_q_r_s[\"offset_mapping\"]))]\n",
    "    tokenized_q_r_s[\"offset_mapping\"][0] = None\n",
    "    return tokenized_q_r_s\n",
    "\n",
    "def format_data_post_rp(q: str, r: str, s: str, ids: str):\n",
    "    q_r_s =  r + '</s>' + q + '</s>' + s \n",
    "    tokenized_q_r_s = tokenizer(q_r_s, return_offsets_mapping=True, padding=\"max_length\", max_length=512, truncation=True)\n",
    "    \n",
    "    cls_q_sep =  r\n",
    "    tokenized_q = tokenizer(cls_q_sep)[\"input_ids\"]\n",
    "    \n",
    "    tokenized_q_r_s[\"example_id\"] = ids\n",
    "    tokenized_q_r_s[\"offset_mapping\"] = [tokenized_q_r_s[\"offset_mapping\"][_] if _ in range(len(tokenized_q)-1) else None for _ in range(len(tokenized_q_r_s[\"offset_mapping\"]))]\n",
    "    tokenized_q_r_s[\"offset_mapping\"][0] = None\n",
    "    return tokenized_q_r_s\n",
    "        \n",
    "def postprocess(data: pd.DataFrame, choice: str):\n",
    "    ids = list(data.id)\n",
    "    Q, R, S = [data[field] for field in [\"q\", \"r\", \"s\"]]\n",
    "    Q, R, S = [list(map(lambda x: x.strip('\"'), y)) for y in [Q, R, S]]\n",
    "    \n",
    "    if choice == 'qp':\n",
    "        data = list(map(format_data_post_qp, Q, R, S, ids))\n",
    "    elif choice == 'rp':\n",
    "        data = list(map(format_data_post_rp, Q, R, S, ids))\n",
    "    input_list, token_list, attention_list, offset, ex_id =[], [], [], [], []\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        input_list.append(data[i][\"input_ids\"])\n",
    "        attention_list.append(data[i][\"attention_mask\"])\n",
    "        offset.append(data[i][\"offset_mapping\"])\n",
    "        ex_id.append(data[i][\"example_id\"])\n",
    "        \n",
    "    data = {\n",
    "        'input_ids': input_list,\n",
    "        'attention_mask': attention_list,\n",
    "        'offset_mapping': offset,\n",
    "        'example_id': ex_id\n",
    "    }\n",
    "    ds = Dataset.from_dict(data)\n",
    "    return ds\n",
    "\n",
    "# load model\n",
    "def getPredictFromCkpt(ckpt: str, choice: str, test_post):\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(f'corrected_models/{args[\"model_name\"]}-{choice}-{args[\"split\"]}-b_{args[\"batch_size\"]}-lr_{args[\"learning_rate\"]}-warm_{args[\"warmup_ratio\"]}-seed_{args[\"seed\"]}-{args[\"special\"]}/checkpoint-{ckpt}')\n",
    "    test_args = TrainingArguments(\n",
    "        output_dir = f'corrected_models/{args[\"model_name\"]}-{choice}-{args[\"split\"]}-b_{args[\"batch_size\"]}-lr_{args[\"learning_rate\"]}-warm_{args[\"warmup_ratio\"]}-seed_{args[\"seed\"]}-{args[\"special\"]}/checkpoint-{ckpt}',\n",
    "        do_train = False,\n",
    "        do_predict = True,\n",
    "        per_device_eval_batch_size = args[\"batch_size\"],\n",
    "        gradient_accumulation_steps=2\n",
    "    )\n",
    "\n",
    "    # init trainer\n",
    "    trainer = Trainer(model = model, args = test_args)\n",
    "    raw_predictions = trainer.predict(test_post)\n",
    "    return raw_predictions\n",
    "\n",
    "# turn raw predictions (start/end span) to strings\n",
    "def postprocess_qa_predictions(examples, features, raw_predictions, choice, n_best_size = 10, max_answer_length = 510):\n",
    "    all_start_logits, all_end_logits = raw_predictions\n",
    "    n_best_size = 10\n",
    "    predictions = collections.OrderedDict()\n",
    "    # Let's loop over all the examples!\n",
    "    for example_index in range(examples.shape[0]):\n",
    "        # Those are the indices of the features associated to the current example.\n",
    "        valid_answers = []\n",
    "        \n",
    "        if choice == 'qp':\n",
    "            context = examples.iloc[example_index][\"q\"][1:-1] #strip \"\n",
    "        elif choice == 'rp':\n",
    "            context = examples.iloc[example_index][\"r\"][1:-1] #strip \"\n",
    "        \n",
    "        # We grab the predictions of the model for this feature.\n",
    "        start_logits = all_start_logits[example_index]\n",
    "        end_logits = all_end_logits[example_index]\n",
    "        # This is what will allow us to map some the positions in our logits to span of texts in the original\n",
    "        # context.\n",
    "        offset_mapping = features[example_index][\"offset_mapping\"]\n",
    "        # Update minimum null prediction.\n",
    "        cls_index = features[example_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
    "        feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "\n",
    "        # Go through all possibilities for the `n_best_size` greater start and end logits.\n",
    "        start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "        end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "        for start_index in start_indexes:\n",
    "            for end_index in end_indexes:\n",
    "                # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
    "                # to part of the input_ids that are not in the context.\n",
    "                if (\n",
    "                    start_index >= len(offset_mapping)\n",
    "                    or end_index >= len(offset_mapping)\n",
    "                    or offset_mapping[start_index] is None\n",
    "                    or offset_mapping[end_index] is None\n",
    "                ):\n",
    "                    continue\n",
    "                # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
    "                if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                    continue\n",
    "\n",
    "                start_char = offset_mapping[start_index][0]\n",
    "                end_char = offset_mapping[end_index][1]\n",
    "                valid_answers.append(\n",
    "                    {\n",
    "                        \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                        \"text\": context[start_char: end_char] # +1 because of the starting \"\n",
    "                    }\n",
    "                )\n",
    "        \n",
    "        if len(valid_answers) > 0:\n",
    "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
    "        else:\n",
    "            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n",
    "            # failure.\n",
    "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
    "        predictions[examples.iloc[example_index][\"id\"]] = best_answer[\"text\"]\n",
    "        \n",
    "    return predictions\n",
    "\n",
    "# make gold csv from valid/ test set data\n",
    "def make_gold_csv(data, output_name):\n",
    "    test_dropped = data.drop(['q', 'r', 's'], axis=1)\n",
    "    test_dropped = test_dropped.fillna('')\n",
    "    test_dropped.to_csv(output_name, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f00d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the evaluation csv\n",
    "make_gold_csv(valid_data,\"split62+2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52722f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# real_predict_test = 1, if we are going to predict the final submission test\n",
    "# real_predict_test = 0, if we are only predicting for evaluation \n",
    "real_predict_test = 0\n",
    "if real_predict_test == 1:\n",
    "    # Get test data for final submission\n",
    "    valid_data = pd.read_csv(\"./dataset/test.csv\")\n",
    "\n",
    "# Leave only unique id\n",
    "test_data_unique = leave_unique_id(valid_data)\n",
    "# Start postprocessing\n",
    "test_qp_post = postprocess(test_data_unique, 'qp')\n",
    "test_rp_post = postprocess(test_data_unique, 'rp')\n",
    "\n",
    "ckpts = list(range(400, 3200, 200))\n",
    "if real_predict_test == 1:\n",
    "    ckpts = ['1600']\n",
    "for ckpt in ckpts:\n",
    "    if real_predict_test == 0:\n",
    "        # Get raw predictions from our model (We use the same checkpoint for both q' r' when evaluating)\n",
    "        raw_predictions_qp = getPredictFromCkpt(ckpt, 'qp', test_qp_post)\n",
    "        raw_predictions_rp = getPredictFromCkpt(ckpt, 'rp', test_rp_post)\n",
    "    elif if real_predict_test == 1:\n",
    "        # Get raw predictions from our model (We found combining different checkpoints of q' models and r' models gives us a better result)\n",
    "        # Checkpoint-1200 of q' and Checkpoint-2400 of r' claims the best result\n",
    "        raw_predictions_qp = getPredictFromCkpt('1200', 'qp', test_qp_post)\n",
    "        raw_predictions_rp = getPredictFromCkpt('2400', 'rp', test_rp_post)\n",
    "    \n",
    "    # The Trainer hides the columns that are not used by the model, so we set them back\n",
    "    test_qp_post.set_format(type=test_qp_post.format[\"type\"], columns=list(test_qp_post.features.keys()))\n",
    "    test_rp_post.set_format(type=test_rp_post.format[\"type\"], columns=list(test_rp_post.features.keys()))\n",
    "    \n",
    "    # Get final predictions\n",
    "    final_predictions_qp = postprocess_qa_predictions(test_data_unique, test_qp_post, raw_predictions_qp.predictions, 'qp')\n",
    "    final_predictions_rp = postprocess_qa_predictions(test_data_unique, test_rp_post, raw_predictions_rp.predictions, 'rp')\n",
    "    \n",
    "    ids, qp, rp = [], [], []\n",
    "    for k,v in final_predictions_qp.items():\n",
    "        ids.append(k)\n",
    "        qp.append(v)\n",
    "    for k,v in final_predictions_rp.items():\n",
    "        rp.append(v)\n",
    "\n",
    "    dict = {'id': ids, \"q'\": qp, \"r'\": rp} \n",
    "    df = pd.DataFrame(dict) \n",
    "    df = df.replace(np.nan, '', regex=True)\n",
    "    \n",
    "    # Save the predictions as csv\n",
    "    if real_predict_test == 1:\n",
    "        df.to_csv(f'./outputs/corrected_models/HALF_predict_test_{args[\"model_name\"]}-{args[\"split\"]}-b_{args[\"batch_size\"]}-lr_{args[\"learning_rate\"]}-warm_{args[\"warmup_ratio\"]}-seed_{args[\"seed\"]}-{args[\"special\"]}-checkpoint-{ckpt}.csv',header=False)\n",
    "    else:\n",
    "        tempname = 'fb-muppet-roberta-base'\n",
    "        path = f'./outputs/corrected_models/{tempname}'\n",
    "        # Check whether the specified path exists or not\n",
    "        isExist = os.path.exists(path)\n",
    "        if not isExist:\n",
    "           # Create a new directory because it does not exist\n",
    "           os.makedirs(path)\n",
    "        df.to_csv(f'./outputs/corrected_models/{tempname}-{args[\"split\"]}-b_{args[\"batch_size\"]}-seed_{args[\"seed\"]}/checkpoint-{ckpt}.csv',header=False)\n",
    "    print(\"Predictions to csv done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e46f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this block for predicting final submission\n",
    "# Choose how to fill na if any, {0: Fill null, 1: Fill whole q or r sequence, 2: Fill with other csv predictions}\n",
    "how_to_fill_na = 1\n",
    "\n",
    "if real_predict_test == 1:\n",
    "    pred_df = pd.read_csv(f'./outputs/corrected_models/HALF_predict_test_muppet-roberta-base-finetuned-squad-{args[\"split\"]}-b_{args[\"batch_size\"]}-lr_{args[\"learning_rate\"]}-warm_{args[\"warmup_ratio\"]}-seed_{args[\"seed\"]}-{args[\"special\"]}-checkpoint-{ckpt}.csv', names=[\"id\", \"q'\", \"r'\"], dtype=str)\n",
    "    valid_data = pd.read_csv(\"./dataset/test.csv\")\n",
    "    \n",
    "    print('Any nas?', pred_df.isna().sum().sum())    \n",
    "    if how_to_fill_na == 0:\n",
    "        pred_df.fillna(value='', inplace=True)\n",
    "    elif how_to_fill_na == 1:\n",
    "        r, _ = np.where(pred_df.isna())\n",
    "        print(r, _)\n",
    "        for i in range(len(r)):\n",
    "            for j in range(valid_data.shape[0]):\n",
    "                if valid_data.iloc[j]['id'] == int(pred_df.iloc[r[i]][0]):\n",
    "                    getq = valid_data.iloc[j][\"q\"]\n",
    "                    getr = valid_data.iloc[j][\"r\"]\n",
    "                    if _[i] == 1:\n",
    "                        pred_df.iloc[r[i]][\"q'\"] = getq\n",
    "                    elif _[i] == 2:\n",
    "                        pred_df.iloc[r[i]][\"r'\"] = getr\n",
    "                    break\n",
    "    elif how_to_fill_na == 2:\n",
    "        helper_df = pd.read_csv('./outputs/corrected_models/REAL_predict_test_muppet-roberta-base-finetuned-squad-6+22-b_8-lr_3e-05-warm_0.06-seed_24-checkpoint-2000.csv', names=[\"id\", \"q\", \"r\"], dtype=str)\n",
    "        r, _ = np.where(pred_df.isna())\n",
    "        print(helper_df.shape[0], helper_df.iloc[0]['id'])\n",
    "        print(r, _)\n",
    "        for i in range(len(r)):\n",
    "            for j in range(helper_df.shape[0]):\n",
    "                if int(helper_df.iloc[j]['id']) == int(pred_df.iloc[r[i]][0]):\n",
    "                    getq = helper_df.iloc[j][\"q\"].replace('\"','')\n",
    "                    getr = helper_df.iloc[j][\"r\"].replace('\"','')\n",
    "                    if _[i] == 1 and getq != '':\n",
    "                        pred_df.iloc[r[i]][\"q'\"] = getq\n",
    "                    elif _[i] == 2 and getr != '':\n",
    "                        pred_df.iloc[r[i]][\"r'\"] = getr\n",
    "                    break\n",
    "    print('Any nas?', pred_df.isna().sum().sum())\n",
    "\n",
    "    pred_df = pred_df.rename({\"q'\": 'q', \"r'\": 'r'}, axis=1)\n",
    "    pred_df.loc[:, ['q', 'r']] = pred_df[['q', 'r']].applymap(lambda s: '\"' + str(s).strip('\"') + '\"')\n",
    "    pred_df.to_csv(f'./outputs/corrected_models/REAL_predict_test_muppet-roberta-base-finetuned-squad-{args[\"split\"]}-b_{args[\"batch_size\"]}-lr_{args[\"learning_rate\"]}-warm_{args[\"warmup_ratio\"]}-seed_{args[\"seed\"]}-checkpoint-{ckpt}.csv', header=True, quotechar='\"', index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dff97f0",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c861eb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Dict\n",
    "from typing import List, Tuple, Any, Union\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from transformers import EvalPrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cba764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_tokenize(text: str, filter_puncts: bool = True) -> List[str]:\n",
    "    punctuations = set([ch for ch in \"!\\\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~\"])\n",
    "    text = text.strip('\"') # NOTE: remove the quotes first\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    if filter_puncts:\n",
    "        tokens = list(filter(lambda t: t not in punctuations, tokens))\n",
    "    return tokens\n",
    "    \n",
    "def longestCommonSubsequence(text1: list, text2: list) -> int:\n",
    "    if len(text2) > len(text1):\n",
    "        text1, text2 = text2, text1\n",
    "\n",
    "    lcs = [[0] * (len(text2) + 1) for _ in range(2)]\n",
    "    for i in range(1, len(text1)+1):\n",
    "        for j in range(1, len(text2)+1):\n",
    "            if text1[i-1] == text2[j-1]:\n",
    "                lcs[i % 2][j] = lcs[(i-1) % 2][j-1] + 1\n",
    "            else:\n",
    "                lcs[i % 2][j] = max(lcs[(i-1) % 2][j], lcs[i % 2][j-1])\n",
    "\n",
    "    return lcs[len(text1) % 2][len(text2)]\n",
    "\n",
    "def compute_lcs_score(pred: list, ans: list) -> float:\n",
    "    intersection = longestCommonSubsequence(pred, ans)\n",
    "    union = len(pred) + len(ans) - intersection\n",
    "    if union == 0:\n",
    "        return 0\n",
    "    lcs_score = intersection / union\n",
    "    if (lcs_score < 0) or (lcs_score) > 1:\n",
    "        raise ValueError(\"LCS score must be between 0 and 1\")\n",
    "    return lcs_score\n",
    "\n",
    "def compute_lcs_scores(pred_df: pd.DataFrame, ans_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ids, qp_scores, rp_scores = list(), list(), list()\n",
    "    for _, prow in pred_df.iterrows():\n",
    "        pid, qp_pred, rp_pred = prow[\"id\"], prow[\"q'\"], prow[\"r'\"]\n",
    "        qp_pred, rp_pred = [nltk_tokenize(pred) for pred in [qp_pred, rp_pred]]\n",
    "        ans_rows = ans_df[ans_df.id == pid]\n",
    "\n",
    "        for _, arow in ans_rows.iterrows():\n",
    "            qp_ans, rp_ans = arow[\"q'\"], arow[\"r'\"]\n",
    "            qp_ans, rp_ans = [nltk_tokenize(ans) for ans in [qp_ans, rp_ans]]\n",
    "            qp_score, rp_score = compute_lcs_score(qp_pred, qp_ans), compute_lcs_score(rp_pred, rp_ans)\n",
    "\n",
    "            for item, l in zip([pid, qp_score, rp_score], [ids, qp_scores, rp_scores]):\n",
    "                l.append(item)\n",
    "\n",
    "    assert ids == ans_df.id.tolist()\n",
    "    lcs_df = pd.DataFrame(data={\n",
    "        \"id\": ids,\n",
    "        \"qp_scores\": qp_scores,\n",
    "        \"rp_scores\": rp_scores\n",
    "    })\n",
    "    return lcs_df\n",
    "\n",
    "def compute_final_score(lcs_df: pd.DataFrame) -> float:\n",
    "    lcs_df[\"total_scores\"] = lcs_df[\"qp_scores\"] + lcs_df[\"rp_scores\"]\n",
    "    max_scores = lcs_df.groupby(\"id\")[\"total_scores\"].max()\n",
    "    final_score = max_scores.sum() / (2 * len(max_scores))\n",
    "    if (final_score < 0) or (final_score > 1):\n",
    "        raise ValueError(\"The final score must be between 0 and 1, please check the implementation.\")\n",
    "    return final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042f80b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "how_to_fill_nas = [0, 1]\n",
    "# Change ckpts if you need to evaluate different ckpts\n",
    "# ckpts = list(range(200, 2600, 200))\n",
    "\n",
    "# Run both fill_na with null and whole sequence of qr \n",
    "for how_to_fill_na in how_to_fill_nas:\n",
    "    for ckpt in ckpts:\n",
    "        pred_df = pd.read_csv(f'./outputs/corrected_models/{tempname}/{args[\"split\"]}-b_{args[\"batch_size\"]}-seed_{args[\"seed\"]}/checkpoint-{ckpt}.csv', names=[\"id\", \"q'\", \"r'\"], dtype=str)\n",
    "        ans_df = pd.read_csv(\"split6+22.csv\", names=[\"id\", \"q'\", \"r'\"], dtype=str)\n",
    "\n",
    "        print('Any nas?', pred_df.isna().sum().sum())\n",
    "        if how_to_fill_na == 0:\n",
    "            pred_df.fillna(value='', inplace=True)\n",
    "        elif how_to_fill_na == 1:\n",
    "            r, _ = np.where(pred_df.isna())\n",
    "            for i in range(len(r)):\n",
    "                for j in range(valid_data.shape[0]):\n",
    "                    if valid_data.iloc[j]['id'] == int(pred_df.iloc[r[i]][0]):\n",
    "                        getq = valid_data.iloc[j][\"q\"]\n",
    "                        getr = valid_data.iloc[j][\"r\"]\n",
    "                        if _[i] == 1:\n",
    "                            pred_df.iloc[r[i]][1] = getq\n",
    "                        elif _[i] == 2:\n",
    "                            pred_df.iloc[r[i]][2] = getr\n",
    "                        break\n",
    "\n",
    "        \n",
    "        # Check whether pred_df has the same nums of evaluation ids\n",
    "        if len(pred_df) != len(ans_df.groupby(\"id\").size()):\n",
    "            raise ValueError(\"The prediction file must have the same number of rows as the number of unique IDs in the answer file\")\n",
    "\n",
    "        # Start evaluating\n",
    "        lcs_df = compute_lcs_scores(pred_df, ans_df) # has len(ans_df) rows of lcs_q' and lcs_r'\n",
    "        final_score = compute_final_score(lcs_df) # derive the final score by \"1/2N (\\sum_i^N(max_j(score_q' + score_r')))\"\n",
    "        # Print score\n",
    "        print(f'# {tempname}-{args[\"split\"]}-ckpt-{ckpt} final score: {final_score}')\n",
    "    print('# ============')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55303072",
   "metadata": {},
   "source": [
    "### Model combination test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa7ebf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import *\n",
    "\n",
    "ans_df = pd.read_csv(\"/nfs/nas-6.1/wclu/AICUP/full_valid.csv\")\n",
    "max_q = 0\n",
    "max_r = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdf38e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/nfs/nas-6.1/wclu/corrected_models/janeel/muppet-roberta-base-finetuned-squad-62+2-b_8-seed_26\"\n",
    "for step in range(200, 3001, 200): \n",
    "    try:\n",
    "        pred_df = pd.read_csv(f\"{path}/checkpoint-{step}.csv\", names=['id', \"q'\", \"r'\"]).fillna(\"\")\n",
    "        q = pred_df[['id', \"q'\"]]\n",
    "        empty_r = pd.DataFrame(data={\"r'\": [\"\" for _ in range(1598)]})\n",
    "        r = pred_df[['id', \"r'\"]]\n",
    "        empty_q = pd.DataFrame(data={\"q'\": [\"\" for _ in range(1598)]})\n",
    "\n",
    "        q_frame=pd.concat([q, empty_r], axis=1).reset_index(drop=True)\n",
    "        r_frame=pd.concat([r, empty_q], axis=1).reset_index(drop=True)\n",
    "\n",
    "        lcs = compute_lcs_scores(q_frame, ans_df)\n",
    "        score_q = compute_final_score(lcs)\n",
    "\n",
    "        lcs = compute_lcs_scores(r_frame, ans_df)\n",
    "        score_r = compute_final_score(lcs)\n",
    "        if score_q >= max_q:\n",
    "            best_q = q\n",
    "            step_q = step\n",
    "            q_path = path\n",
    "            max_q = score_q\n",
    "        if score_r >= max_r:\n",
    "            best_r = r\n",
    "            step_r = step\n",
    "            r_path = path\n",
    "            max_r = score_r\n",
    "        print(f\"ckpt{step} q_score: {score_q} r_score: {score_r}\")\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "best_pred = pd.concat([best_q, best_r[\"r'\"]], axis=1).reset_index(drop=True)\n",
    "lcs = compute_lcs_scores(best_pred, ans_df)\n",
    "score = compute_final_score(lcs)\n",
    "print(f\"model for q: {q_path}_{step_q}, score: {max_q}\")\n",
    "print(f\"model for r: {r_path}_{step_r}, score: {max_r}\")\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
